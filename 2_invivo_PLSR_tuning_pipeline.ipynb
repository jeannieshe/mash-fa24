{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3165e22e-fabc-456c-84fa-3868df39e7d5",
   "metadata": {},
   "source": [
    "# Tuning PLSR models and testing them against external datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce18ae8-1d47-430a-86da-a5d303056136",
   "metadata": {},
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63011544-da6c-4c80-8db6-3262ec033cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up\n",
    "# imports\n",
    "import pyreadr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from scipy.stats import pearsonr\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# calculate the mean of the pearson coefficients of multiple variables\n",
    "def pair_pearsonr(x, y, axis=0): # this will allow us to take the pearson coefficient across two variables\n",
    "    mx = np.mean(x, axis=axis, keepdims=True)\n",
    "    my = np.mean(y, axis=axis, keepdims=True)\n",
    "    xm, ym = x-mx, y-my\n",
    "    r_num = np.add.reduce(xm * ym, axis=axis)\n",
    "    r_den = np.sqrt((xm*xm).sum(axis=axis) * (ym*ym).sum(axis=axis))\n",
    "    r = r_num / r_den\n",
    "    return r\n",
    "\n",
    "# plotting setup\n",
    "# Use LaTeX for graphs' text\n",
    "plt.rc('text', usetex=True)\n",
    "# Use the serif font\n",
    "plt.rc('font', family='serif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf087ceb-da8d-4ec2-ad2c-06e73f27f0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data, access pandas df via key \"None\"\n",
    "X_Govaere = pyreadr.read_r(\"datasets/X_Govaere.rds\")[None]\n",
    "X_Hoang = pyreadr.read_r(\"datasets/X_Hoang.rds\")[None]\n",
    "X_Pantano = pyreadr.read_r(\"datasets/X_Pantano.rds\")[None]\n",
    "\n",
    "# actual, clinical results\n",
    "Y_Govaere = pyreadr.read_r(\"datasets/Y_Govaere.rds\")[None]\n",
    "Y_Hoang = pyreadr.read_r(\"datasets/Y_Hoang.rds\")[None]\n",
    "Y_Pantano = pyreadr.read_r(\"datasets/Y_Pantano.rds\")[None]\n",
    "\n",
    "# viper metric, predicted results\n",
    "Y_viper_Govaere = pyreadr.read_r(\"datasets/viper_X_Govaere.rds\")[None]\n",
    "Y_viper_Hoang = pyreadr.read_r(\"datasets/viper_X_Hoang.rds\")[None]\n",
    "Y_viper_Pantano = pyreadr.read_r(\"datasets/viper_X_Pantano.rds\")[None]\n",
    "\n",
    "# all_results = pd.read_csv(\"datasets/PLSR_pearson_coefficients.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d25fb9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fibrosis\n",
       "2    51\n",
       "3    47\n",
       "1    43\n",
       "0    39\n",
       "4    14\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_Govaere[\"fibrosis\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2a14b199-ff9a-4d87-95ef-ff88cfa80fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique NAS scores and their corresponding frequency:\n",
      "['(0.0, 32)', '(1.0, 12)', '(2.0, 9)', '(3.0, 11)', '(4.0, 13)', '(5.0, 18)', '(6.0, 12)', '(7.0, 9)']\n",
      "Unique Fibrosis scores and their corresponding frequency:\n",
      "['(0.0, 57)', '(1.0, 25)', '(2.0, 25)', '(3.0, 5)', '(4.0, 4)']\n"
     ]
    }
   ],
   "source": [
    "# setup tuning procedure\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=5)\n",
    "num_lvs = range(2, 11)\n",
    "\n",
    "# select our training dataset\n",
    "# TODO: CHANGE THE FOLLOWING LINES\n",
    "training_dataset = \"Pantano\"\n",
    "X = X_Pantano\n",
    "Y = Y_Pantano\n",
    "\n",
    "test1 = \"Govaere\"\n",
    "test2 = \"Hoang\"\n",
    "\n",
    "X_test1 = X_Govaere\n",
    "X_test2 = X_Hoang\n",
    "Y_test1 = Y_Govaere\n",
    "Y_test2 = Y_Hoang\n",
    "\n",
    "unique, freq = np.unique(Y.iloc[:,0], return_counts=True)\n",
    "print(f\"Unique NAS scores and their corresponding frequency:\\n{[f'{(i,j)}' for i, j in zip(unique, freq)]}\")\n",
    "unique, freq = np.unique(Y.iloc[:,1], return_counts=True)\n",
    "print(f\"Unique Fibrosis scores and their corresponding frequency:\\n{[f'{(i,j)}' for i, j in zip(unique, freq)]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b107c8-062e-4364-8a67-288a219d4256",
   "metadata": {},
   "source": [
    "## Perform cross validation to find ideal number of latent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "56c5c87d-3f8d-4409-85b4-ad4adcffbb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[156], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m Y_test \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39miloc[test_index]\n\u001b[1;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m PLSRegression(n_components\u001b[38;5;241m=\u001b[39mlatent_var, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 16\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, Y_train)\n\u001b[1;32m     17\u001b[0m Y_test_hat \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# take the mean in order to be able to generalize the behavior on both phenotypes\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/cross_decomposition/_pls.py:649\u001b[0m, in \u001b[0;36mPLSRegression.fit\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, Y):\n\u001b[1;32m    632\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit model to data.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \n\u001b[1;32m    634\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;124;03m        Fitted model.\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X, Y)\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;66;03m# expose the fitted attributes `x_scores_` and `y_scores_`\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_scores_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_x_scores\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/cross_decomposition/_pls.py:234\u001b[0m, in \u001b[0;36m_PLS.fit\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit model to data.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03m    Fitted model.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    233\u001b[0m check_consistent_length(X, Y)\n\u001b[0;32m--> 234\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    235\u001b[0m     X, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy, ensure_min_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    236\u001b[0m )\n\u001b[1;32m    237\u001b[0m Y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m    238\u001b[0m     Y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    239\u001b[0m )\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:875\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    869\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    870\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas.DataFrame with sparse columns found.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    871\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt will be converted to a dense numpy array.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    872\u001b[0m         )\n\u001b[1;32m    874\u001b[0m dtypes_orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtypes)\n\u001b[0;32m--> 875\u001b[0m pandas_requires_conversion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    876\u001b[0m     _pandas_dtype_needs_early_conversion(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m dtypes_orig\n\u001b[1;32m    877\u001b[0m )\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(dtype_iter, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;28;01mfor\u001b[39;00m dtype_iter \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[1;32m    879\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mresult_type(\u001b[38;5;241m*\u001b[39mdtypes_orig)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:876\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    869\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    870\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas.DataFrame with sparse columns found.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    871\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt will be converted to a dense numpy array.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    872\u001b[0m         )\n\u001b[1;32m    874\u001b[0m dtypes_orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtypes)\n\u001b[1;32m    875\u001b[0m pandas_requires_conversion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m--> 876\u001b[0m     _pandas_dtype_needs_early_conversion(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m dtypes_orig\n\u001b[1;32m    877\u001b[0m )\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(dtype_iter, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;28;01mfor\u001b[39;00m dtype_iter \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[1;32m    879\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mresult_type(\u001b[38;5;241m*\u001b[39mdtypes_orig)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:678\u001b[0m, in \u001b[0;36m_pandas_dtype_needs_early_conversion\u001b[0;34m(pd_dtype)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;66;03m# Check these early for pandas versions without extension dtypes\u001b[39;00m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[0;32m--> 678\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    679\u001b[0m     is_bool_dtype,\n\u001b[1;32m    680\u001b[0m     is_float_dtype,\n\u001b[1;32m    681\u001b[0m     is_integer_dtype,\n\u001b[1;32m    682\u001b[0m )\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_bool_dtype(pd_dtype):\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;66;03m# bool and extension booleans need early conversion because __array__\u001b[39;00m\n\u001b[1;32m    686\u001b[0m     \u001b[38;5;66;03m# converts mixed dtype dataframes into object dtypes\u001b[39;00m\n\u001b[1;32m    687\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pearson_coeff_lvs = [] # containing tuples (num_lv, avg pearson coeff)\n",
    "\n",
    "for latent_var in num_lvs:\n",
    "    \n",
    "    pearson_coeff = []\n",
    "\n",
    "    # index into fibrosis column only because we need the correct dim for finding indices, and want to have as equal of splits as possible\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(X, Y[Y.columns[1]])):\n",
    "\n",
    "        X_train = X.iloc[train_index]\n",
    "        X_test = X.iloc[test_index]\n",
    "        Y_train = Y.iloc[train_index]\n",
    "        Y_test = Y.iloc[test_index]\n",
    "\n",
    "        model = PLSRegression(n_components=latent_var, scale=False)\n",
    "        model.fit(X_train, Y_train)\n",
    "        Y_test_hat = model.predict(X_test)\n",
    "\n",
    "        # take the mean in order to be able to generalize the behavior on both phenotypes\n",
    "        pearson_coeff.append(np.mean(pair_pearsonr(Y_test.values, Y_test_hat)))\n",
    "    \n",
    "    # evaluate the hyperparameter based on the average pearson coeff across all 10 folds\n",
    "    pearson_coeff_lvs.append((latent_var, sum(pearson_coeff)/len(pearson_coeff)))\n",
    "\n",
    "# print the result of the hyperparameter optimization\n",
    "print(f'From 10-fold cross validation on training dataset {training_dataset}, {max(pearson_coeff_lvs, key=lambda x: x[1])[0]} latent variables achieves an average Pearson coeff of {max(pearson_coeff_lvs, key=lambda x: x[1])[1]}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "300ec2bb-7834-4b16-9bba-22e160c31660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model for Fold 0 saved as models/Pantano_PLSR_fold_0.pkl\n",
      "Model for Fold 1 saved as models/Pantano_PLSR_fold_1.pkl\n",
      "Model for Fold 2 saved as models/Pantano_PLSR_fold_2.pkl\n",
      "Model for Fold 3 saved as models/Pantano_PLSR_fold_3.pkl\n",
      "Model for Fold 4 saved as models/Pantano_PLSR_fold_4.pkl\n",
      "Model for Fold 5 saved as models/Pantano_PLSR_fold_5.pkl\n",
      "Model for Fold 6 saved as models/Pantano_PLSR_fold_6.pkl\n",
      "Model for Fold 7 saved as models/Pantano_PLSR_fold_7.pkl\n",
      "Model for Fold 8 saved as models/Pantano_PLSR_fold_8.pkl\n",
      "Model for Fold 9 saved as models/Pantano_PLSR_fold_9.pkl\n"
     ]
    }
   ],
   "source": [
    "# save the model so we can run it again\n",
    "# this time, store the pearson's coeff for training and validation sets\n",
    "\n",
    "best_lv = max(pearson_coeff_lvs, key=lambda x: x[1])[0]\n",
    "\n",
    "latent_var = best_lv # our best performing hyperparameter\n",
    "train_pearson_coeff = []\n",
    "test_pearson_coeff = []\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, Y[Y.columns[1]])):\n",
    "    \n",
    "    X_train = X.iloc[train_index]\n",
    "    X_test = X.iloc[test_index]\n",
    "    Y_train = Y.iloc[train_index]\n",
    "    Y_test = Y.iloc[test_index]\n",
    "\n",
    "    model = PLSRegression(n_components=latent_var, scale=False)\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    Y_train_hat = model.predict(X_train) \n",
    "    train_pearson_coeff.append(np.mean(pair_pearsonr(Y_train.values, Y_train_hat)))\n",
    "    \n",
    "    Y_test_hat = model.predict(X_test) \n",
    "    test_pearson_coeff.append(np.mean(pair_pearsonr(Y_test.values, Y_test_hat)))\n",
    "    \n",
    "    filename = f\"models/{training_dataset}_PLSR_fold_{i}.pkl\"\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "    \n",
    "    print(f\"Model for Fold {i} saved as {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "1429b4a2-06f3-4060-9df4-da9e5f9cf449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 0 from models/Pantano_PLSR_fold_0.pkl\n",
      "Loaded model 1 from models/Pantano_PLSR_fold_1.pkl\n",
      "Loaded model 2 from models/Pantano_PLSR_fold_2.pkl\n",
      "Loaded model 3 from models/Pantano_PLSR_fold_3.pkl\n",
      "Loaded model 4 from models/Pantano_PLSR_fold_4.pkl\n",
      "Loaded model 5 from models/Pantano_PLSR_fold_5.pkl\n",
      "Loaded model 6 from models/Pantano_PLSR_fold_6.pkl\n",
      "Loaded model 7 from models/Pantano_PLSR_fold_7.pkl\n",
      "Loaded model 8 from models/Pantano_PLSR_fold_8.pkl\n",
      "Loaded model 9 from models/Pantano_PLSR_fold_9.pkl\n"
     ]
    }
   ],
   "source": [
    "# load all models\n",
    "loaded_models = []\n",
    "for i in range(0, 10):\n",
    "    filename = f\"models/{training_dataset}_PLSR_fold_{i}.pkl\"\n",
    "    with open(filename, 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "        loaded_models.append(model)\n",
    "    print(f\"Loaded model {i} from {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab44f23-0883-425b-8f62-2f5da31f3144",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Test with the best model on external and shuffled datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4042d78-d6a8-40f3-bc94-2db805573a8b",
   "metadata": {},
   "source": [
    "## Trying to recreate the all_coeff dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a3d70d",
   "metadata": {},
   "source": [
    "# Hide the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79dfc81e-850f-4278-b08e-50faaf17b48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the model to create the total predicted scores\n",
    "# run on the other two external test sets and the shuffled training dataset\n",
    "# TODO: CHANGE THE FOLLOWING 6 LINES\n",
    "X_test1 = X_Hoang\n",
    "X_test2 = X_Pantano\n",
    "Y_test1 = Y_Hoang\n",
    "Y_test2 = Y_Pantano\n",
    "test1 = \"Hoang\"\n",
    "test2 = \"Pantano\"\n",
    "\n",
    "train_pearson_coeff = []\n",
    "validation_pearson_coeff = []\n",
    "test1_pearson_coeff = []\n",
    "test2_pearson_coeff = []\n",
    "shuffled_pearson_coeff = []\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, Y[Y.columns[1]])):\n",
    "\n",
    "    X_train = X.iloc[train_index]\n",
    "    X_val = X.iloc[test_index]\n",
    "    Y_train = Y.iloc[train_index]\n",
    "    Y_val = Y.iloc[test_index]\n",
    "\n",
    "    # model = loaded_models[i]\n",
    "    model = PLSRegression(n_components=latent_var, scale=False)\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    Y_train_hat = model.predict(X_train)\n",
    "    train_pearson_coeff.append(np.mean(pair_pearsonr(Y_train.values, Y_train_hat)))\n",
    "    \n",
    "    Y_val_hat = model.predict(X_val)\n",
    "    validation_pearson_coeff.append(np.mean(pair_pearsonr(Y_val.values, Y_val_hat)))\n",
    "    \n",
    "    Y_pred = model.predict(X_test1)\n",
    "    test1_pearson_coeff.append(np.mean(pair_pearsonr(Y_test1.values, Y_pred)))\n",
    "\n",
    "    Y_pred = model.predict(X_test2)\n",
    "    test2_pearson_coeff.append(np.mean(pair_pearsonr(Y_test2.values, Y_pred)))\n",
    "\n",
    "    Y_pred = model.predict(X_shuffled)\n",
    "    shuffled_pearson_coeff.append(np.mean(pair_pearsonr(Y.values, Y_pred)))\n",
    "\n",
    "    model = PLSRegression(n_components=latent_var, scale=False)\n",
    "    model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cbf711",
   "metadata": {},
   "source": [
    "# Show the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "1ce0ca3d-71f6-4a4b-b3ad-b1bdc919a125",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 0\n",
      "Evaluating fold 1\n",
      "Evaluating fold 2\n",
      "Evaluating fold 3\n",
      "Evaluating fold 4\n",
      "Evaluating fold 5\n",
      "Evaluating fold 6\n",
      "Evaluating fold 7\n",
      "Evaluating fold 8\n",
      "Evaluating fold 9\n"
     ]
    }
   ],
   "source": [
    "# creating a better all_coeff dataframe\n",
    "# TODO: CHANGE THE FOLLOWING 6 LINES\n",
    "# test1 = \"Govaere\"\n",
    "# test2 = \"Pantano\"\n",
    "\n",
    "# X_test1 = X_Govaere\n",
    "# X_test2 = X_Pantano\n",
    "# Y_test1 = Y_Govaere\n",
    "# Y_test2 = Y_Pantano\n",
    "X_shuffled = X.apply(lambda col: np.random.permutation(col.values), axis=0)\n",
    "X_shuffled.index = X.index # ensure the gene names are maintained\n",
    "\n",
    "results = [] # initialize list of all results\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, Y[Y.columns[1]])):\n",
    "\n",
    "    # split data\n",
    "    X_train = X.iloc[train_index]\n",
    "    X_val = X.iloc[test_index]\n",
    "    Y_train = Y.iloc[train_index]\n",
    "    Y_val = Y.iloc[test_index]\n",
    "\n",
    "    partitions = [[X_train, X_val, X_test1, X_test2, X_shuffled], [Y_train, Y_val, Y_test1, Y_test2, Y], [\"Train\", \"Validation\", test1, test2, \"Shuffled\"]]\n",
    "\n",
    "    print(f'Evaluating fold {i}')\n",
    "    model = loaded_models[i]\n",
    "\n",
    "    for j in range(len(partitions[0])):\n",
    "        \n",
    "        Y_hat = model.predict(partitions[0][j])\n",
    "        corr_nas, _ = pearsonr(partitions[1][j].values[:,0], Y_hat[:,0]) # NAS correlation\n",
    "        corr_fib, _ = pearsonr(partitions[1][j].values[:,1], Y_hat[:,1]) # fibrosis correlation\n",
    "        corr_mean = np.mean(pair_pearsonr(partitions[1][j].values, Y_hat)) # mean correlation\n",
    "        \n",
    "        results.append({\n",
    "            \"TuningDataset\": training_dataset,\n",
    "            \"Fold\": f\"Fold{i+1}\",\n",
    "            \"Partition\": partitions[2][j],\n",
    "            \"Phenotype\": \"Fibrosis\", \n",
    "            \"Pearson_Coefficient\": corr_fib\n",
    "        })\n",
    "        results.append({\n",
    "            \"TuningDataset\": training_dataset,\n",
    "            \"Fold\": f\"Fold{i+1}\",\n",
    "            \"Partition\": partitions[2][j],\n",
    "            \"Phenotype\": \"NAS\",\n",
    "            \"Pearson_Coefficient\": corr_nas\n",
    "        })\n",
    "        results.append({\n",
    "            \"TuningDataset\": training_dataset,\n",
    "            \"Fold\": f\"Fold{i+1}\",\n",
    "            \"Partition\": partitions[2][j],\n",
    "            \"Phenotype\": \"Combined\",\n",
    "            \"Pearson_Coefficient\": corr_mean\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ac305a9b-f9fb-4002-a505-c3e31223e395",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pd.DataFrame(results)\n",
    "# results_df[(results_df[\"Phenotype\"] == \"NAS\") & (results_df[\"Partition\"] == \"Pantano\")]\n",
    "# results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "cc9de344",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.concat([g, h, p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "00cf3b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "test.to_csv(\"datasets/PLSR_pearson_coefficients.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e73208-ec0a-4bdb-947a-deb511776ec4",
   "metadata": {},
   "source": [
    "## Plot with Mann-Whitney test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29f94ac7-553a-4e11-83dd-f988fd188bf1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_pearsons_coeff' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m combinations \u001b[38;5;241m=\u001b[39m [(x, \u001b[38;5;241m4\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m combo \u001b[38;5;129;01min\u001b[39;00m combinations:\n\u001b[0;32m----> 5\u001b[0m     data1 \u001b[38;5;241m=\u001b[39m all_pearsons_coeff\u001b[38;5;241m.\u001b[39miloc[:,combo[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m      6\u001b[0m     data2 \u001b[38;5;241m=\u001b[39m all_pearsons_coeff\u001b[38;5;241m.\u001b[39miloc[:,combo[\u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# calculate significance\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_pearsons_coeff' is not defined"
     ]
    }
   ],
   "source": [
    "significant_combinations = []\n",
    "\n",
    "combinations = [(x, 4) for x in [1, 2, 3]]\n",
    "for combo in combinations:\n",
    "    data1 = all_pearsons_coeff.iloc[:,combo[0]]\n",
    "    data2 = all_pearsons_coeff.iloc[:,combo[1]]\n",
    "    \n",
    "    # calculate significance\n",
    "    U, p = stats.mannwhitneyu(data1, data2, alternative='two-sided')\n",
    "    if p < 0.05:\n",
    "        significant_combinations.append([combo, p])\n",
    "\n",
    "# create a set of axes\n",
    "ax = plt.axes()\n",
    "# create a boxplot on the axes\n",
    "bp = ax.boxplot(all_pearsons_coeff, patch_artist=True)\n",
    "# graph title\n",
    "ax.set_title(f\"Tuning PLSR Model with {training_dataset} in-vivo Training Data\", fontsize=16)\n",
    "# label y-axis\n",
    "ax.set_ylabel(\"Pearson Coefficient\")\n",
    "\n",
    "xticklabels = all_pearsons_coeff.columns.tolist()\n",
    "ax.set_xticklabels(xticklabels)\n",
    "\n",
    "# Change the colour of the boxes to Seaborn's 'pastel' palette\n",
    "colors = sns.color_palette('colorblind')\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "# Colour of the median lines\n",
    "plt.setp(bp['medians'], color='k')\n",
    "\n",
    "# Get the y-axis limits\n",
    "bottom, top = ax.get_ylim()\n",
    "y_range = top - bottom\n",
    "\n",
    "# Significance bars\n",
    "for i, significant_combination in enumerate(significant_combinations):\n",
    "    # Columns corresponding to the datasets of interest\n",
    "    # add one because the df indexing is 0 indexed, but plot is 1 indexed\n",
    "    x1 = significant_combination[0][0] + 1 \n",
    "    x2 = significant_combination[0][1] + 1\n",
    "    # What level is this bar among the bars above the plot?\n",
    "    level = len(significant_combinations) - i\n",
    "    # Plot the bar\n",
    "    bar_height = (y_range * 0.07 * level) + top\n",
    "    bar_tips = bar_height - (y_range * 0.02)\n",
    "    plt.plot(\n",
    "        [x1, x1, x2, x2],\n",
    "        [bar_tips, bar_height, bar_height, bar_tips], lw=1, c='k'\n",
    "    )\n",
    "    # Significance level\n",
    "    p = significant_combination[1]\n",
    "    if p < 0.001:\n",
    "        sig_symbol = '***'\n",
    "    elif p < 0.01:\n",
    "        sig_symbol = '**'\n",
    "    elif p < 0.05:\n",
    "        sig_symbol = '*'\n",
    "    text_height = bar_height + (y_range * 0.01)\n",
    "    plt.text((x1 + x2) * 0.5, text_height, sig_symbol, ha='center', va='bottom', c='k')\n",
    "\n",
    "plt.savefig(f'graphs/PLSR_tuning_with_{training_dataset}.png', dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84199023-0ebd-4c67-b2e2-8b8b81b66e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(1, 4), 0.00018267179110955002],\n",
       " [(2, 4), 0.00018267179110955002],\n",
       " [(3, 4), 0.00018267179110955002]]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "significant_combinations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
